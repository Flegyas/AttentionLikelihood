{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attlike.utils import (\n",
    "    load_transformer,\n",
    "    tokenize,\n",
    "    perturb,\n",
    "    dataset_format,\n",
    "    data_info,\n",
    "    load_data,\n",
    "    LikelihoodMode,\n",
    "    sample_encode,\n",
    ")\n",
    "import functools\n",
    "from typing import Sequence\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from datasets import DatasetDict, Dataset\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy import Language\n",
    "import spacy.cli as spacy_down\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpacyManager:\n",
    "    _lang2model = {\n",
    "        \"en\": \"en_core_web_sm\",\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def instantiate(cls, language: str) -> Language:\n",
    "        model_name = SpacyManager._lang2model[language]\n",
    "\n",
    "        try:\n",
    "            pipeline: Language = spacy.load(model_name)\n",
    "        except Exception as e:  # noqa\n",
    "            spacy_down.download(model_name)\n",
    "            pipeline: Language = spacy.load(model_name)\n",
    "\n",
    "        return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader import Lemma, Synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_LEMMA_SEP: str = \",\"\n",
    "\n",
    "lang2model = {\n",
    "    lang: SpacyManager.instantiate(language=lang)\n",
    "    for lang in [\n",
    "        \"en\",\n",
    "    ]\n",
    "}\n",
    "spacy2wn_lang = {\"en\": \"eng\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import dataclasses\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Sample:\n",
    "    synset_id: str\n",
    "    lemma: str\n",
    "    sentence: str\n",
    "    pos: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synset2samples = {}\n",
    "\n",
    "for synset in tqdm(wn.all_synsets(), desc=\"Iterating synsets\"):\n",
    "    \"\"\"For each synset, we iterate over all the examples and lemmas.\n",
    "    If the lemma is in the example, and the example is long enough, we add it to the list of samples.\"\"\"\n",
    "    synset: Synset\n",
    "\n",
    "    synset_samples = []\n",
    "    for lang, spacy_model in lang2model.items():\n",
    "        wn_lang: str = spacy2wn_lang[lang]\n",
    "        examples = synset.examples(lang=wn_lang)\n",
    "        lemmas = synset.lemma_names(lang=wn_lang)\n",
    "\n",
    "        tokenized_examples = [[token.text for token in spacy_model.tokenizer(example)] for example in examples]\n",
    "        for lemma in lemmas:\n",
    "            if \"_\" in lemma:\n",
    "                continue\n",
    "            for example_index in range(len(tokenized_examples)):\n",
    "                tokenized_example = tokenized_examples[example_index]\n",
    "                if len(tokenized_example) < 5:\n",
    "                    continue\n",
    "                if Counter(tokenized_example).get(lemma, 0) == 1:\n",
    "                    synset_samples.append(\n",
    "                        Sample(\n",
    "                            synset_id=synset.name(),\n",
    "                            lemma=lemma,\n",
    "                            sentence=examples[example_index],\n",
    "                            pos=synset.pos(),\n",
    "                        )\n",
    "                    )\n",
    "    if len(synset_samples) > 0:\n",
    "        synset2samples[synset.name()] = synset_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(samples) for samples in synset2samples.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_dataset = Dataset.from_list(\n",
    "    [dataclasses.asdict(sample) for samples in synset2samples.values() for sample in samples]\n",
    ")\n",
    "real_dataset = real_dataset.map(function=lambda _, index: {\"index\": index}, batched=True, with_indices=True)\n",
    "real_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders: Sequence[str] = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"roberta-base\",\n",
    "    \"xlm-roberta-base\",\n",
    "    \"roberta-large\",\n",
    "    \"distilbert-base-uncased\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder2data = {}\n",
    "for encoder_name in encoders:\n",
    "    \"\"\"For each encoder, we tokenize the dataset.\"\"\"\n",
    "    _, tokenizer = load_transformer(transformer_name=encoder_name)\n",
    "\n",
    "    encoder_data = real_dataset.map(\n",
    "        functools.partial(tokenize, tokenizer=tokenizer, encoder_name=encoder_name),\n",
    "        num_proc=1,\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        desc=f\"{encoder_name} tokenization\",\n",
    "    )\n",
    "\n",
    "    dataset_format(encoder_data)\n",
    "    encoder2data[encoder_name] = encoder_data\n",
    "encoder2data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"We filter out the samples that are not tokenized correctly. \"\"\"\n",
    "encoder2data = {\n",
    "    encoder_name: encoder_data.filter(\n",
    "        function=lambda start_lemma_index, end_lemma_index: start_lemma_index != -1 and end_lemma_index != -1,\n",
    "        input_columns=[\"start_lemma_index\", \"end_lemma_index\"],\n",
    "    )\n",
    "    for encoder_name, encoder_data in encoder2data.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kept_indices = set.intersection(*[set(encoder_data[\"index\"]) for encoder_data in encoder2data.values()])\n",
    "len(kept_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder2data = {\n",
    "    encoder_name: encoder_data.filter(function=lambda index: index.item() in kept_indices, input_columns=[\"index\"])\n",
    "    for encoder_name, encoder_data in encoder2data.items()\n",
    "}\n",
    "for encoder_name, encoder_data in encoder2data.items():\n",
    "    dataset_format(encoder_data)\n",
    "\n",
    "encoder2data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" We encode each sample in the dataset with each encoder. \"\"\"\n",
    "for encoder_name, encoder_data in encoder2data.items():\n",
    "    encoder, tokenizer = load_transformer(transformer_name=encoder_name)\n",
    "    encoder = encoder.to(DEVICE)\n",
    "    encoder_data = encoder_data.map(\n",
    "        functools.partial(sample_encode, encoder=encoder, tokenizer=tokenizer, encoder_name=encoder_name),\n",
    "        num_proc=1,\n",
    "        batched=False,\n",
    "        with_indices=True,\n",
    "        desc=f\"{encoder_name} sample encoding\",\n",
    "    )\n",
    "    encoder2data[encoder_name] = encoder_data\n",
    "    encoder.cpu()\n",
    "\n",
    "encoder2data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_core.common import PROJECT_ROOT\n",
    "\n",
    "DATA_DIR: Path = PROJECT_ROOT / \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data = DatasetDict(encoder2data)\n",
    "real_data.save_to_disk(DATA_DIR / \"real\")\n",
    "real_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data = load_data(DATA_DIR / \"real\")\n",
    "real_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for encoder_name, encoder_data in real_data.items():\n",
    "    encoder_data = encoder_data.map(\n",
    "        function=data_info,\n",
    "        num_proc=8,\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        with_indices=True,\n",
    "        desc=f\"{encoder_name} data info\",\n",
    "        input_columns=[\"attention\", \"start_lemma_index\", \"end_lemma_index\"],\n",
    "    )\n",
    "    dataset_format(encoder_data)\n",
    "    real_data[encoder_name] = encoder_data\n",
    "\n",
    "real_data.save_to_disk(PROJECT_ROOT / \"real\")\n",
    "real_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data = load_data(\"data/real\")\n",
    "real_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_modes = list(LikelihoodMode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" We change the likelihood of each sample in the dataset for each encoder.\"\"\"\n",
    "encoder2likelihood = {}\n",
    "for encoder_name, encoder_data in real_data.items():\n",
    "    all_columns = encoder_data.column_names\n",
    "    encoder, tokenizer = load_transformer(transformer_name=encoder_name)\n",
    "    encoder = encoder.to(DEVICE)\n",
    "\n",
    "    encoder2likelihood[encoder_name] = encoder_data.map(\n",
    "        function=functools.partial(\n",
    "            perturb,\n",
    "            # tokenizer=tokenizer,\n",
    "            encoder=encoder,\n",
    "            likelihood_modes=likelihood_modes,\n",
    "        ),\n",
    "        num_proc=1,\n",
    "        batched=False,\n",
    "        desc=f\"{encoder_name} changing likelihood\",\n",
    "        input_columns=[\n",
    "            \"sentence_ids\",\n",
    "            \"attention_mask\",\n",
    "            \"start_lemma_index\",\n",
    "            \"end_lemma_index\",\n",
    "            \"lemma_ids\",\n",
    "        ],\n",
    "        remove_columns=[\n",
    "            x\n",
    "            for x in all_columns\n",
    "            if x\n",
    "            not in {\n",
    "                \"synset_id\",\n",
    "                \"lemma\",\n",
    "                \"sentence\",\n",
    "                \"pos\",\n",
    "                \"index\",\n",
    "                \"sentence_ids\",\n",
    "                \"sentence_special_mask\",\n",
    "                \"attention_mask\",\n",
    "                \"lemma_ids\",\n",
    "                \"start_lemma_index\",\n",
    "                \"end_lemma_index\",\n",
    "            }\n",
    "            and x in all_columns\n",
    "        ],\n",
    "    )\n",
    "    encoder.cpu()\n",
    "\n",
    "    dataset_format(encoder2likelihood[encoder_name])\n",
    "encoder2likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" We re-encode each perturbed sample in the dataset with each encoder, for each likelihood.\"\"\"\n",
    "likelihood2encoder2data = {}\n",
    "for likelihood_mode in likelihood_modes:\n",
    "    for encoder_name, encoder_data in encoder2likelihood.items():\n",
    "        all_columns = encoder_data.column_names\n",
    "        encoder, tokenizer = load_transformer(transformer_name=encoder_name)\n",
    "        encoder = encoder.to(DEVICE)\n",
    "\n",
    "        encoder_data = encoder_data.map(\n",
    "            functools.partial(\n",
    "                sample_encode,\n",
    "                tokenizer=tokenizer,\n",
    "                encoder=encoder,\n",
    "                encoder_name=encoder_name,\n",
    "                likelihood_mode=likelihood_mode,\n",
    "            ),\n",
    "            num_proc=1,\n",
    "            batched=False,\n",
    "            with_indices=True,\n",
    "            desc=f\"{encoder_name} sample encoding\",\n",
    "            remove_columns=[x for x in LikelihoodMode if x != likelihood_mode and x in all_columns],\n",
    "        )\n",
    "        encoder.cpu()\n",
    "\n",
    "        dataset_format(encoder_data)\n",
    "\n",
    "        likelihood2encoder2data.setdefault(likelihood_mode, DatasetDict())[encoder_name] = encoder_data\n",
    "likelihood2encoder2data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood2encoder2data[\"real\"] = real_data\n",
    "likelihood2encoder2data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood2encoder2data = DatasetDict({k: DatasetDict(v) for k, v in likelihood2encoder2data.items()})\n",
    "likelihood2encoder2data.save_to_disk(DATA_DIR / \"likelihood2encoder2data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood2encoder2data = DatasetDict(\n",
    "    {\n",
    "        likelihood_path.name: DatasetDict.load_from_disk(likelihood_path)\n",
    "        for likelihood_path in (DATA_DIR / \"likelihood2encoder2data\").iterdir()\n",
    "        if not likelihood_path.name.endswith(\".json\")\n",
    "    }\n",
    ")\n",
    "likelihood2encoder2data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for likelihood_mode, encoder2data in likelihood2encoder2data.items():\n",
    "    for encoder_name, encoder_data in encoder2data.items():\n",
    "        encoder_data = encoder_data.map(\n",
    "            function=data_info,\n",
    "            num_proc=1,\n",
    "            batched=True,\n",
    "            batch_size=1000,\n",
    "            with_indices=True,\n",
    "            desc=f\"{encoder_name} data info\",\n",
    "            input_columns=[\"attention\", \"start_lemma_index\", \"end_lemma_index\"],\n",
    "        )\n",
    "        dataset_format(encoder_data)\n",
    "\n",
    "        likelihood2encoder2data[likelihood_mode][encoder_name] = encoder_data\n",
    "\n",
    "likelihood2encoder2data.save_to_disk(DATA_DIR / \"likelihoods\")\n",
    "likelihood2encoder2data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attlike",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e190c64035ea994edb35a95c6b38ed865775c81e6eda414744506182cf5d7912"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
